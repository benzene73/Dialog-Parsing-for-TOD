{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-20T02:59:27.457236Z","iopub.execute_input":"2023-04-20T02:59:27.457933Z","iopub.status.idle":"2023-04-20T02:59:27.468108Z","shell.execute_reply.started":"2023-04-20T02:59:27.457892Z","shell.execute_reply":"2023-04-20T02:59:27.467087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dialogue parsing model for task-oriented dialogue systems\nUsing pre-trained models BART\nYour model should take user utterance as input and predict the parsed output in the given\nformat. As a starting point, you can consider the task as a sequence generation where you\ngenerate the parsed output given the user input. You can then fine-tune BART on the task-specific data.\nYou are expected to decide how to represent input and outputs to PLMs during training and inference.\n\nAll train, evaluation and sample test data are given here. All files use utf-8 encoding.\n1. There are 31k training samples given in the train.jsonl file. The format of the data is\njsonl where each line represents a training sample serialized in json format.\nThis includes incorporating a combination of different input fields like input, history, user_lists, etc..\n2. There are 9.2k evaluation samples given in the dev.jsonl file. It follows jsonl format\nsame as in training data except “pattern” field\n3. Sample test input and output data are given in the sample_test.jsonl and\nsample_output.txt files. Following a realistic setting where the linguistic pattern for a\nsample is not known beforehand, the field “pattern” is not available in the test data.\nThese i/o formats are to be followed strictly.\n\nJsonl file entries are as follows:\nYou can read about intents and slot-value\n1. input: the user utterances\n2. history: a list of past user and system utterances\n3. user_lists: List of user’s list. A user list is a named collection of items curated by the\nuser. See example below.\n4. user_notes: List of user's notes. A user note consists of a title and text. See example\nbelow\n5. user_contacts: List of user's contacts\n6. output: String indicating parsing output\n7. pattern: Linguistic pattern in the user utterance\n\nSample output:\nConsider the output string “Send_digital_object ( medium « Message » recipient\nPersonal_contact ( person « my mom » ) )”. Here, Send_digital_object is the intent and\n(medium, Message) and (recipient Personal_contact, (person, my mom) are slot-value pairs.\nNote that the data has nested slot-value pairs as well as can be seen for (person, my mom).\nThere are a total 34 intents and 303 slot types in the dataset.\n\nSample input:\n{\"input\": \"Email um please email sarah@gmail.com\", \"history\": [{\"user_query\": \"Could you create a list for me named Costco\", \"response_text\": \"Sorry, it looks like you already have a list with that name. Do you still want to make a new one?\"}, {\"user_query\": \"Add pears and bananas on it\", \"response_text\": \"Got it, I added pears and bananas.\"}, {\"user_query\": \"apples\", \"response_text\": \"Alright, what do you want to call it?\"}, {\"user_query\": \"berries\", \"response_text\": \"Sure, I made a list called \\\"Berries\\\" and added apples.\"}, {\"user_query\": \"bread\", \"response_text\": \"Got it, what do you want to call it?\"}], \"user_lists\": [{\"name\": \"shopping\", \"items\": [\"bananas\", \"shoes\"]}, {\"name\": \"office supplies\", \"items\": [\"backpacks\", \"peanuts\", \"pens\"]}, {\"name\": \"nut\", \"items\": [\"almond\", \"acorn\"]}, {\"name\": \"Kroger shopping\", \"items\": [\"red wine vinegar\"]}, {\"name\": \"photo\", \"items\": [\"grad pictures\"]}, {\"name\": \"fruit salad\", \"items\": [\"strawberries\"]}], \"user_notes\": [{\"name\": \"Grocery List\", \"content\": \"\"}, {\"name\": \"pick up Kennedi at 12\", \"content\": \"\"}], \"user_contacts\": [\"Eman\", \"Berry\", \"Vera\", \"Clarence\", \"Phil Leeper\", \"Toya\", \"HR\", \"Kade\", \"Ariana\", \"Dr Robertson\"], \"output\": \"Send_digital_object ( medium \\u00ab email \\u00bb recipient Contactable_entity ( contact_id Email_address ( id_form \\u00ab sarah@gmail.com \\u00bb ) ) )\", \"pattern\": \"disfluency\"}\n","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import List, Optional, Dict\nfrom tqdm import tqdm\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\n\nfrom transformers import (\n    T5ForConditionalGeneration,\n    BartModel,\n    BartForConditionalGeneration,\n    T5Tokenizer,\n    BartTokenizer,\n    BartConfig,\n    AdamW,\n    get_linear_schedule_with_warmup,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T17:02:32.260385Z","iopub.execute_input":"2023-04-24T17:02:32.260666Z","iopub.status.idle":"2023-04-24T17:02:44.331286Z","shell.execute_reply.started":"2023-04-24T17:02:32.260637Z","shell.execute_reply":"2023-04-24T17:02:44.329987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T17:03:31.90639Z","iopub.execute_input":"2023-04-24T17:03:31.90677Z","iopub.status.idle":"2023-04-24T17:03:31.914106Z","shell.execute_reply.started":"2023-04-24T17:03:31.906737Z","shell.execute_reply":"2023-04-24T17:03:31.911596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(file_path):\n    data = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:12:56.019215Z","iopub.execute_input":"2023-04-20T05:12:56.020311Z","iopub.status.idle":"2023-04-20T05:12:56.027425Z","shell.execute_reply.started":"2023-04-20T05:12:56.020266Z","shell.execute_reply":"2023-04-20T05:12:56.02639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/assistant-data/train.jsonl\"\ndev_path = \"/kaggle/input/assistant-data/dev.jsonl\"\ntrain_data = load_data(train_path)\ndev_data = load_data(dev_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:12:58.027848Z","iopub.execute_input":"2023-04-20T05:12:58.028221Z","iopub.status.idle":"2023-04-20T05:12:59.761042Z","shell.execute_reply.started":"2023-04-20T05:12:58.028189Z","shell.execute_reply":"2023-04-20T05:12:59.759985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(train_path)\ni = 0\nfor line in f:\n    if i > 0:\n        break\n    i += 1\n    print(line)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:59:32.199033Z","iopub.execute_input":"2023-04-20T02:59:32.199689Z","iopub.status.idle":"2023-04-20T02:59:32.208784Z","shell.execute_reply.started":"2023-04-20T02:59:32.199648Z","shell.execute_reply":"2023-04-20T02:59:32.207495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\nmax_input_length = 512\nmax_output_length = 128","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:13:02.641864Z","iopub.execute_input":"2023-04-20T05:13:02.642501Z","iopub.status.idle":"2023-04-20T05:13:02.647783Z","shell.execute_reply.started":"2023-04-20T05:13:02.642462Z","shell.execute_reply":"2023-04-20T05:13:02.646584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the model\n# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n# #model = BartModel.from_pretrained(\"facebook/bart-large\")\n# tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n#model = BartModel.from_pretrained(\"facebook/bart-large\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n# Define the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nloss_fn.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:13:05.519794Z","iopub.execute_input":"2023-04-20T05:13:05.520515Z","iopub.status.idle":"2023-04-20T05:13:20.484309Z","shell.execute_reply.started":"2023-04-20T05:13:05.520474Z","shell.execute_reply":"2023-04-20T05:13:20.483223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(data_sample):\n    # Implement the preprocessing function\n    input_text = data_sample['input']\n    history_text = ' '.join([f\"user_query: {entry['user_query']}. response_text: {entry['response_text']}.\" for entry in data_sample['history']])\n    user_lists_text = ' '.join([f\"{user_list['name']}: {', '.join(user_list['items'])}.\" for user_list in data_sample['user_lists']])\n    user_notes_text = ' '.join([f\"{user_note['name']}: {user_note['content']}.\" for user_note in data_sample['user_notes']])\n    user_contacts_text = ', '.join(data_sample['user_contacts'])\n\n    return f\"{input_text} [history] {history_text} [user_lists] {user_lists_text} [user_notes] {user_notes_text} [user_contacts] {user_contacts_text}\"\n\ndef preprocess_output(data_sample):\n    output = data_sample[\"output\"]\n    pattern = data_sample[\"pattern\"]\n    if pattern == \"\":\n        pattern = \"fluency\"\n    return f\"{output} [pattern] {pattern}\"","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:20:17.865553Z","iopub.execute_input":"2023-04-20T05:20:17.865995Z","iopub.status.idle":"2023-04-20T05:20:17.876089Z","shell.execute_reply.started":"2023-04-20T05:20:17.865961Z","shell.execute_reply":"2023-04-20T05:20:17.874811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\ni = 1\nfor sample in train_data:\n    if sample[\"pattern\"] == \"\":\n        l.append(i+1)\n    i += 1\nprint(len(l))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:59:52.162952Z","iopub.execute_input":"2023-04-20T02:59:52.1636Z","iopub.status.idle":"2023-04-20T02:59:52.512687Z","shell.execute_reply.started":"2023-04-20T02:59:52.163561Z","shell.execute_reply":"2023-04-20T02:59:52.511606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_inp(data):\n    inp, out = [], []\n    for sample in data:\n        inp.append(preprocess_input(sample))\n        out.append(preprocess_output(sample))\n        out.append(sample[\"output\"])\n    return inp, out\n\ndef preprocessor_dev(data):\n    inp, out = [], []\n    for sample in data:\n        inp.append(preprocess_input(sample))\n        #out.append(preprocess_output(sample))\n        out.append(sample[\"output\"])\n    return inp, out\n\ndef preprocessor_inp(data):\n    inp, out = [], []\n    for sample in data:\n        inp.append(preprocess_input(sample))\n        out.append(preprocess_output(sample))\n    return inp, out","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:20:22.969252Z","iopub.execute_input":"2023-04-20T05:20:22.97006Z","iopub.status.idle":"2023-04-20T05:20:22.976677Z","shell.execute_reply.started":"2023-04-20T05:20:22.97002Z","shell.execute_reply":"2023-04-20T05:20:22.975557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = preprocessor_inp(train_data)\nX_dev, Y_dev = preprocessor_dev(dev_data)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:20:26.5905Z","iopub.execute_input":"2023-04-20T05:20:26.590984Z","iopub.status.idle":"2023-04-20T05:20:26.93627Z","shell.execute_reply.started":"2023-04-20T05:20:26.590938Z","shell.execute_reply":"2023-04-20T05:20:26.935231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tokenized = tokenizer(X_train,padding='max_length', truncation=True, max_length=max_input_length, return_tensors='pt')\nY_train_tokenized = tokenizer(Y_train,padding='max_length', truncation=True, max_length=max_output_length, return_tensors='pt')\nX_dev_tokenized = tokenizer(X_dev,padding='max_length', truncation=True, max_length=max_input_length, return_tensors='pt')\nY_dev_tokenized = tokenizer(Y_dev,padding='max_length', truncation=True, max_length=max_output_length, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:20:29.132895Z","iopub.execute_input":"2023-04-20T05:20:29.13387Z","iopub.status.idle":"2023-04-20T05:21:23.761549Z","shell.execute_reply.started":"2023-04-20T05:20:29.133815Z","shell.execute_reply":"2023-04-20T05:21:23.760419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_rows = torch.randperm(1024)[:10]\nprint(rand_rows)\nY_train_tokenized[\"attention_mask\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:00:46.440452Z","iopub.execute_input":"2023-04-20T03:00:46.440915Z","iopub.status.idle":"2023-04-20T03:00:46.469634Z","shell.execute_reply.started":"2023-04-20T03:00:46.440876Z","shell.execute_reply":"2023-04-20T03:00:46.468441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, input_encodings, output_encodings, size = 0):\n        self.size = size\n        if size:\n            rows = torch.randperm(len(output_encodings[\"input_ids\"]))[:size]\n            self.input_ids = input_encodings[\"input_ids\"][rows]\n            self.attention_mask = input_encodings[\"attention_mask\"][rows]\n            self.outputs = output_encodings[\"input_ids\"][rows]\n        else:\n            self.input_ids = input_encodings[\"input_ids\"]\n            self.attention_mask = input_encodings[\"attention_mask\"]\n            self.outputs = output_encodings[\"input_ids\"]\n    def __len__(self):\n        if self.size:\n            return self.size\n        else:\n            return len(self.input_ids)\n    def __getitem__(self,idx):\n        item = dict()\n        item[\"input_ids\"] = self.input_ids[idx].to(device)\n        item[\"attention_mask\"] = self.attention_mask[idx].to(device)\n        item[\"labels\"] = self.outputs[idx].to(device)\n        return item","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:22:59.484159Z","iopub.execute_input":"2023-04-20T05:22:59.485202Z","iopub.status.idle":"2023-04-20T05:22:59.494152Z","shell.execute_reply.started":"2023-04-20T05:22:59.485145Z","shell.execute_reply":"2023-04-20T05:22:59.493039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = dataset(X_train_tokenized, Y_train_tokenized)\n# dev_dataset = dataset(X_dev_tokenized, Y_dev_tokenized)\ntrain_dataset = dataset(X_train_tokenized, Y_train_tokenized)\ndev_dataset = dataset(X_dev_tokenized, Y_dev_tokenized)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:23:03.407083Z","iopub.execute_input":"2023-04-20T05:23:03.407791Z","iopub.status.idle":"2023-04-20T05:23:03.43719Z","shell.execute_reply.started":"2023-04-20T05:23:03.407753Z","shell.execute_reply":"2023-04-20T05:23:03.436199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:23:07.100862Z","iopub.execute_input":"2023-04-20T05:23:07.10174Z","iopub.status.idle":"2023-04-20T05:23:07.10862Z","shell.execute_reply.started":"2023-04-20T05:23:07.101701Z","shell.execute_reply":"2023-04-20T05:23:07.107026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saved_model = torch.load(\"/kaggle/input/todp-2/model1.pth\") \n#model.load_state_dict(saved_model)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:00:46.528997Z","iopub.execute_input":"2023-04-20T03:00:46.529379Z","iopub.status.idle":"2023-04-20T03:00:59.78911Z","shell.execute_reply.started":"2023-04-20T03:00:46.529344Z","shell.execute_reply":"2023-04-20T03:00:59.787738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 5e-5\nnum_epochs = 4\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n\nnum_training_steps = num_epochs * len(train_dataloader)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:23:11.280739Z","iopub.execute_input":"2023-04-20T05:23:11.281336Z","iopub.status.idle":"2023-04-20T05:23:11.28853Z","shell.execute_reply.started":"2023-04-20T05:23:11.2813Z","shell.execute_reply":"2023-04-20T05:23:11.287445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, dev_dataloader):\n    model.eval()\n    pred_outputs = []\n    gold_outputs = []\n    with torch.no_grad():\n        for batch in dev_dataloader:\n                    input_ids, attention_mask, labels = [batch[x] for x in batch]\n        #outputs = model(input_ids.view(1,-1), attention_mask=attention_mask.view(1,-1), labels=labels.view(1,-1))\n                    batch_outputs = model.generate(input_ids, num_beams=2, min_length=0, max_length=max_output_length, pad_token_id = tokenizer.pad_token_id)\n                    #print(input_ids)\n                    #print(batch_outputs)\n        #outputs = model.generate(input_ids.view(1,-1), num_beams=2, min_length=0, max_length=max_output_length)\n                    predictions = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n                    predictions = [pred.split(\" [pattern]\")[0] for pred in predictions]\n                    pred_outputs.extend(predictions)\n                    gold_outputs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n    return gold_outputs, pred_outputs","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:51:23.66977Z","iopub.execute_input":"2023-04-20T05:51:23.67044Z","iopub.status.idle":"2023-04-20T05:51:23.67891Z","shell.execute_reply.started":"2023-04-20T05:51:23.670391Z","shell.execute_reply":"2023-04-20T05:51:23.677845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\n\ndef parse(tokens):\n    if \"(\" not in tokens:\n        assert \")\" not in tokens\n        ret = dict()\n        start = 0\n        mid = 0\n        for ii, tok in enumerate(tokens):\n            if tok == \"«\":\n                mid = ii\n            elif tok == \"»\":\n                key = ' '.join(tokens[start:mid])\n                val = ' '.join(tokens[mid + 1:ii])\n                ret[key] = val\n                start = mid = ii + 1\n        return ret\n\n    st = tokens.index(\"(\")\n    outer_key = ' '.join(tokens[0:st])\n    assert tokens[-1] == \")\", \" \".join(tokens)\n\n    level = 0\n    last = st + 1\n    ret = dict()\n    for ii in range(st + 1, len(tokens) - 1, 1):\n        tok = tokens[ii]\n        if tok == \"»\" and level == 0:\n            rr = parse(tokens[last:ii + 1])\n            ret.update(rr)\n            last = ii + 1\n        elif tok == \"(\":\n            level += 1\n        elif tok == \")\":\n            level -= 1\n            if level == 0:\n                rr = parse(tokens[last:ii + 1])\n                ret.update(rr)\n                last = ii + 1\n\n    return {outer_key: ret}\n\n\ndef load_jsonl(fname):\n    data = []\n    with open(fname, 'r', encoding='utf-8') as fp:\n        for line in fp:\n            data.append(json.loads(line.strip()))\n\n    return data\n\n\ndef per_sample_metric(gold, pred):\n    ret = dict()\n    ret['accuracy'] = int(gold == pred)\n\n    get_intent = lambda x: x.split('(', 1)[0].strip()\n    gintent = get_intent(gold)\n    pintent = get_intent(pred)\n    ret['intent_accuracy'] = int(gintent == pintent)\n\n    parse_correct = 1\n    try:\n        _ = parse(pred.split())\n    except:\n        parse_correct = 0\n    ret['parsing_accuracy'] = parse_correct\n\n    return ret\n\n\ndef compute_metrics(golds, preds):\n    assert len(golds) == len(preds), \"Different number of samples in data and prediction.\"\n\n    #golds = [x['output'] for x in data]\n\n    metrics = [per_sample_metric(gold, pred) for gold, pred in zip(golds, preds)]\n    final_metrics = dict()\n    mnames = list(metrics[0].keys())\n    for key in mnames:\n        final_metrics[key] = sum([met[key] for met in metrics]) / len(golds)\n    \n    return final_metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T06:30:38.485423Z","iopub.execute_input":"2023-04-20T06:30:38.486638Z","iopub.status.idle":"2023-04-20T06:30:38.502569Z","shell.execute_reply.started":"2023-04-20T06:30:38.486597Z","shell.execute_reply":"2023-04-20T06:30:38.501414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_dataloader, dev_dataloader, num_epochs, scheduler, optimizer):\n    for epoch in tqdm(range(num_epochs)):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        model.train()\n        total_loss = 0\n        for batch in train_dataloader:\n            input_ids, attention_mask, labels = [batch[x] for x in batch]\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            total_loss += loss.item()\n\n        print(f\"Training loss: {total_loss / len(train_dataloader)}\")\n#         model.eval()\n#         total_loss = 0\n#         with torch.no_grad():\n#             for batch in dev_dataloader:\n#                 input_ids, attention_mask, labels = [batch[x] for x in batch]\n#                 outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#                 loss = outputs.loss\n\n#                 total_loss += loss.item()\n        gold_outputs, pred_outputs = test(model, dev_dataloader)\n        print(\"Validating\")\n        print(compute_metrics(gold_outputs, pred_outputs))\n\n#         print(f\"Validation loss: {total_loss / len(dev_dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:23:20.780085Z","iopub.execute_input":"2023-04-20T05:23:20.780582Z","iopub.status.idle":"2023-04-20T05:23:20.789453Z","shell.execute_reply.started":"2023-04-20T05:23:20.780545Z","shell.execute_reply":"2023-04-20T05:23:20.787196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(model, train_dataloader, dev_dataloader, num_epochs, scheduler, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:23:24.068199Z","iopub.execute_input":"2023-04-20T05:23:24.068637Z","iopub.status.idle":"2023-04-20T05:28:39.677471Z","shell.execute_reply.started":"2023-04-20T05:23:24.068598Z","shell.execute_reply":"2023-04-20T05:28:39.676331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_dataset[0][\"input_ids\"].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:05:26.695129Z","iopub.status.idle":"2023-04-20T03:05:26.695693Z","shell.execute_reply.started":"2023-04-20T03:05:26.69542Z","shell.execute_reply":"2023-04-20T03:05:26.695447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = dev_dataset[0]\ninput_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\ninput_ids.view(1,-1).shape","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:05:26.69774Z","iopub.status.idle":"2023-04-20T03:05:26.698256Z","shell.execute_reply.started":"2023-04-20T03:05:26.697987Z","shell.execute_reply":"2023-04-20T03:05:26.698012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gold_outputs, pred_outputs = test(model, dev_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:51:27.716059Z","iopub.execute_input":"2023-04-20T05:51:27.71668Z","iopub.status.idle":"2023-04-20T06:01:43.726804Z","shell.execute_reply.started":"2023-04-20T05:51:27.716644Z","shell.execute_reply":"2023-04-20T06:01:43.725726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(pred_outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:05:26.705233Z","iopub.status.idle":"2023-04-20T03:05:26.70572Z","shell.execute_reply.started":"2023-04-20T03:05:26.705469Z","shell.execute_reply":"2023-04-20T03:05:26.705494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(gold_outputs[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T06:30:32.434266Z","iopub.execute_input":"2023-04-20T06:30:32.435059Z","iopub.status.idle":"2023-04-20T06:30:32.440642Z","shell.execute_reply.started":"2023-04-20T06:30:32.435018Z","shell.execute_reply":"2023-04-20T06:30:32.439424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(pred_outputs[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T06:30:34.526187Z","iopub.execute_input":"2023-04-20T06:30:34.527115Z","iopub.status.idle":"2023-04-20T06:30:34.533561Z","shell.execute_reply.started":"2023-04-20T06:30:34.527059Z","shell.execute_reply":"2023-04-20T06:30:34.532395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(compute_metrics(gold_outputs, pred_outputs))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T06:30:42.999281Z","iopub.execute_input":"2023-04-20T06:30:42.999994Z","iopub.status.idle":"2023-04-20T06:30:43.076738Z","shell.execute_reply.started":"2023-04-20T06:30:42.999957Z","shell.execute_reply":"2023-04-20T06:30:43.075655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model1.pth\")","metadata":{},"execution_count":null,"outputs":[]}]}